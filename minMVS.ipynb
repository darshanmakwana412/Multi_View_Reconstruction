{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907813b-856f-4a99-bdca-da2d548c1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from PIL import Image, ImageEnhance\n",
    "from typing import Optional\n",
    "\n",
    "import trimesh\n",
    "import pyrender\n",
    "from pyrender.constants import RenderFlags\n",
    "from tqdm.notebook import tqdm\n",
    "import maxflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439becb1-91d0-4496-bac9-eaf8a560a84c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc053a79-8bfc-4b78-a7d8-711b402b8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pose(img: np.ndarray):\n",
    "    img_size = img.shape[:2]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_corners, img_ids, rejected_corners = detector.detectMarkers(gray)\n",
    "    \n",
    "    img_corners, img_ids, rejected_corners, recovered_ids = detector.refineDetectedMarkers(\n",
    "        img,\n",
    "        board,\n",
    "        img_corners,\n",
    "        img_ids,\n",
    "        rejected_corners\n",
    "    )\n",
    "    \n",
    "    obj_points, img_points = board.matchImagePoints(img_corners, img_ids)\n",
    "    \n",
    "    retval, rvec, tvec = cv2.solvePnP(\n",
    "        obj_points,\n",
    "        img_points,\n",
    "        cameraMatrix,\n",
    "        distCoeffs\n",
    "    )\n",
    "\n",
    "    rvec, tvec = cv2.solvePnPRefineLM(\n",
    "        obj_points,\n",
    "        img_points,\n",
    "        cameraMatrix,\n",
    "        distCoeffs,\n",
    "        rvec,\n",
    "        tvec\n",
    "    )\n",
    "    \n",
    "    R_oc, _ = cv2.Rodrigues(rvec)\n",
    "    R_wc = T @ R_oc.T\n",
    "    t_wc = -R_wc @ tvec\n",
    "    return R_wc, t_wc, rvec, tvec\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#{:02X}{:02X}{:02X}'.format(*rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f25955-0b8f-46a4-aee1-074513b39a9e",
   "metadata": {},
   "source": [
    "## Scene Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14e984-a612-40bf-911e-820255efc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(v: np.ndarray) -> np.ndarray:\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "class Scene:\n",
    "    def __init__(self, mesh, image_width: int = 640, image_height: int = 480) -> None:\n",
    "\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "        \n",
    "        naruto_mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        naruto_pose = np.eye(4)\n",
    "        naruto_pose[1, 3] = -1\n",
    "        \n",
    "        self.pyrender_scene = pyrender.Scene(ambient_light=[0.5, 0.5, 0.5])\n",
    "        self.pyrender_scene.add(naruto_mesh, pose=naruto_pose)\n",
    "        \n",
    "        self.renderer = pyrender.OffscreenRenderer(image_width, image_height)\n",
    "        camera = pyrender.PerspectiveCamera(yfov = np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.pyrender_scene.add_node(self.camera_node)\n",
    "\n",
    "        self.point_clouds = []\n",
    "\n",
    "        self.T = np.array([\n",
    "            [1, 0, 0],\n",
    "            [0, -1, 0],\n",
    "            [0, 0, -1]\n",
    "        ])\n",
    "        \n",
    "    def add_points(self, points: np.ndarray, radius: float = 0.01, color: list[float] = [0, 0, 0.5]) -> None:\n",
    "        sm = trimesh.creation.uv_sphere(radius=radius)\n",
    "        sm.visual.vertex_colors = color\n",
    "        tfs = np.tile(np.eye(4), (len(points), 1, 1))\n",
    "        tfs[:,:3,3] = points\n",
    "        pts = pyrender.Mesh.from_trimesh(sm, poses=tfs)\n",
    "        node = self.pyrender_scene.add(pts)\n",
    "        self.point_clouds.append(node)\n",
    "\n",
    "    def remove_all_points(self) -> None:\n",
    "        for ptsc in self.point_clouds:\n",
    "            self.pyrender_scene.remove_node(ptsc)\n",
    "        self.point_clouds = []\n",
    "        \n",
    "    def sample(self, alpha: Optional[float] = None, beta: Optional[float] = None, gamma: Optional[float] = None, return_depth: bool = False) -> [Image.Image, np.ndarray, np.ndarray]:\n",
    "        rand = np.random.uniform(0, 360, (3)) * np.pi / 180\n",
    "        if alpha == None:\n",
    "            alpha = rand[0]\n",
    "        else:\n",
    "            alpha *= np.pi / 180\n",
    "        if beta == None:\n",
    "            beta = rand[1]\n",
    "        else:\n",
    "            beta *= np.pi / 180\n",
    "        if gamma ==  None:\n",
    "            gamma = rand[2]\n",
    "        else:\n",
    "            gamma *= np.pi / 180\n",
    "        Rx = np.array([\n",
    "                            [1, 0, 0, 0],\n",
    "                            [0, np.cos(alpha), -np.sin(alpha), 0],\n",
    "                            [0, np.sin(alpha), np.cos(alpha), 0],\n",
    "                            [0, 0, 0, 1]]\n",
    "                        )\n",
    "        Ry = np.array([\n",
    "                            [np.cos(beta), 0, np.sin(beta), 0],\n",
    "                            [0, 1, 0, 0],\n",
    "                            [-np.sin(beta), 0, np.cos(beta), 0],\n",
    "                            [0, 0, 0, 1]]\n",
    "                        )\n",
    "        Rz = np.array([\n",
    "                            [np.cos(gamma), -np.sin(gamma), 0, 0],\n",
    "                            [np.sin(gamma), np.cos(gamma), 0, 0],\n",
    "                            [0, 0, 1, 0],\n",
    "                            [0, 0, 0, 1]]\n",
    "                        )\n",
    "        camera_pose = Rx @ Ry @ Rz @ np.array([\n",
    "                            [1, 0, 0, 0],\n",
    "                            [0, 1, 0, 0],\n",
    "                            [0, 0, 1, 2],\n",
    "                            [0, 0, 0, 1]]\n",
    "                        )\n",
    "        self.pyrender_scene.set_pose(self.camera_node, camera_pose)\n",
    "        color, depth = self.renderer.render(self.pyrender_scene)\n",
    "        image = Image.fromarray(color)\n",
    "        enhancer = ImageEnhance.Brightness(image)\n",
    "        image = enhancer.enhance(2.0)\n",
    "        if not return_depth:\n",
    "            depth[depth > 0] = 1\n",
    "        return image, camera_pose, depth\n",
    "\n",
    "    def generate_samples(self, n_samples: int):\n",
    "        images = []\n",
    "        poses = []\n",
    "        masks = []\n",
    "        for _ in range(n_samples):\n",
    "            image, camera_pose, depth = self.sample()\n",
    "            images.append(image)\n",
    "            poses.append(camera_pose)\n",
    "            masks.append(depth)\n",
    "        return images, poses, masks\n",
    "\n",
    "    def intrinsics_matrix(self):\n",
    "\n",
    "        P = self.camera_node.camera.get_projection_matrix()\n",
    "        fx = P[0, 0] * self.image_width / 2\n",
    "        fy = P[1, 1] * self.image_height / 2\n",
    "        cx = (1.0 - P[0, 2]) * self.image_width / 2\n",
    "        cy = (1.0 + P[1, 2]) * self.image_height / 2\n",
    "    \n",
    "        K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
    "    \n",
    "        return K\n",
    "\n",
    "mesh = trimesh.load(\"./Naruto/Naruto.obj\", force=\"mesh\")\n",
    "scene = Scene(mesh = mesh)\n",
    "images, poses, masks = scene.generate_samples(n_samples=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880e3e3-11cc-4c78-84c1-15a0f94c27d1",
   "metadata": {},
   "source": [
    "## Generating Visual Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3c0b4-1e04-44ee-a1ae-3797a6e241d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reso = 300\n",
    "\n",
    "K = scene.intrinsics_matrix()\n",
    "\n",
    "_fx = K[0, 0]\n",
    "_fy = K[1, 1]\n",
    "_cx = K[0, 2]\n",
    "_cy = K[1, 2]\n",
    "\n",
    "cam_pos = np.array([0, 0, 2])\n",
    "bounds = np.array([\n",
    "    [-0.810964,  0.001889, -0.810964],\n",
    "    [ 0.810959,  1.7243  ,  0.810959]\n",
    "])\n",
    "bounds[:, 1] -= 1\n",
    "n_vxls = np.array([reso, reso, reso])\n",
    "volume = np.zeros(n_vxls, dtype=np.uint8)\n",
    "intensities = np.zeros((*list(n_vxls), 3), dtype=np.float16)\n",
    "step = (bounds[1] - bounds[0]) / n_vxls\n",
    "\n",
    "xi = np.arange(n_vxls[0])\n",
    "yi = np.arange(n_vxls[1])\n",
    "zi = np.arange(n_vxls[2])\n",
    "indices = np.stack(np.meshgrid(xi, yi, zi, indexing='ij'), axis=-1)\n",
    "\n",
    "centers = bounds[0] + step / 2.0 + step * indices\n",
    "centers = centers.reshape(-1, 3)\n",
    "\n",
    "for image, pose, mask in tqdm(zip(images, poses, masks), total=len(images)):\n",
    "    \n",
    "    [x, y, z] = scene.T @ np.linalg.inv(pose[:3, :3]) @ centers.T + cam_pos[:, None]\n",
    "    x_proj = (_cx + np.rint(x / z * _fx)).astype(np.int32)\n",
    "    y_proj = (_cy + np.rint(y / z * _fy)).astype(np.int32)\n",
    "    \n",
    "    H, W = mask.shape\n",
    "    valid = (x_proj >= 0) & (x_proj < W) & (y_proj >= 0) & (y_proj < H)\n",
    "    x_proj = np.clip(x_proj, 0, W-1)\n",
    "    y_proj = np.clip(y_proj, 0, H-1)\n",
    "    \n",
    "    valid_voxels = (valid & (mask[y_proj, x_proj] > 0)).reshape(n_vxls)\n",
    "    voxel_indices = indices[valid_voxels]\n",
    "    \n",
    "    x_idx, y_idx, z_idx = voxel_indices.T\n",
    "    volume[x_idx, y_idx, z_idx] += 1\n",
    "    I = (np.array(image)[y_proj, x_proj].astype(np.float16) * valid.astype(np.float16)[:, None]).reshape((*list(n_vxls), 3))\n",
    "    intensities += I\n",
    "\n",
    "intensities /= len(images)\n",
    "intensities = np.clip(np.rint(intensities), a_min=0, a_max=255).astype(int)\n",
    "\n",
    "obj_mask = volume >= len(images)\n",
    "\n",
    "data = {\n",
    "    \"version\": \"Voxel Builder 4.5.1\",\n",
    "    \"project\": {\n",
    "        \"name\": \"naruto\",\n",
    "        \"voxels\": int(np.sum(obj_mask))\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"voxels\": \";\".join([f\"{v[0]},{v[1]},{v[2]},#EE4B2B,1\" for v in zip(*np.where(obj_mask))])\n",
    "    }\n",
    "}\n",
    "with open('model.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729514ba-1b86-461e-9c34-eb629a216728",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_volume = np.zeros_like(volume, dtype=np.uint8)\n",
    "offsets = np.array([\n",
    "    [dx, dy, dz]\n",
    "    for dx in [-1, 0, 1]\n",
    "    for dy in [-1, 0, 1]\n",
    "    for dz in [-1, 0, 1]\n",
    "])\n",
    "for x, y, z in zip(*np.where(volume >= len(images))):\n",
    "    new_volume[tuple(np.clip(np.array([x, y, z]) + offsets, a_min=0, a_max=reso-1).T)] = 1\n",
    "data = {\n",
    "    \"version\": \"Voxel Builder 4.5.1\",\n",
    "    \"project\": {\n",
    "        \"name\": \"naruto\",\n",
    "        \"voxels\": int(np.sum(new_volume))\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"voxels\": \";\".join([\n",
    "            f\"{v[0]},{v[1]},{v[2]},#EE4B2B,1\"\n",
    "            for v in zip(*np.where(new_volume > 0))\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "with open('model.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a9f34-4a40-4ab4-b335-776ca1378d64",
   "metadata": {},
   "source": [
    "## Refining the Visual Hull\n",
    "\n",
    "ref: [Multi View Stereo via Volumetric Graph Cut](https://www.cs.jhu.edu/~misha/Fall13b/Papers/Vogiatzis05.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd1b4b-4b6d-4c44-834b-c2bf6391d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumetricGraph:\n",
    "    def __init__(self, volume, centers, intensities, voxel_size, _lambda=1e6):\n",
    "        self.volume = volume\n",
    "        self.centers = centers\n",
    "        self.intensities = intensities\n",
    "        self.voxel_size = voxel_size\n",
    "        self._lambda = _lambda\n",
    "\n",
    "        print(\"Initializing Hull Surface\")\n",
    "        self.init_hull_surf()\n",
    "        print(\"Initializing Graph\")\n",
    "        self.init_graph()\n",
    "\n",
    "    def init_hull_surf(self):\n",
    "        self.hull_surf = np.zeros_like(self.volume)\n",
    "        nx, ny, nz = self.volume.shape\n",
    "        for x in range(nx):\n",
    "            for y in range(ny):\n",
    "                for z in range(nz):\n",
    "                    if self.volume[x, y, z] > 0:\n",
    "                        self.hull_surf[x, y, z] = 1\n",
    "                        if x > 0 and x < nx - 1 and (self.volume[x-1, y, z] == 0 or self.volume[x+1, y, z] == 0):\n",
    "                            self.hull_surf[x, y, z] = 2\n",
    "                        if y > 0 and y < ny -1 and (self.volume[x, y-1, z] == 0 or self.volume[x, y+1, z] == 0):\n",
    "                            self.hull_surf[x, y, z] = 2\n",
    "                        if z > 0 and z < nz -1 and (self.volume[x, y, z-1] == 0 or self.volume[x, y, z+1] == 0):\n",
    "                            self.hull_surf[x, y, z] = 2\n",
    "\n",
    "    def init_graph(self):\n",
    "        self.graph = maxflow.Graph[float]()\n",
    "        self.node_ids = self.graph.add_grid_nodes(self.volume.shape)\n",
    "        self.add_t_links()\n",
    "        self.add_n_links()\n",
    "\n",
    "    def add_t_links(self):\n",
    "        sink_inds = np.where(self.hull_surf == 2)\n",
    "        sink_nodes = self.node_ids[sink_inds]\n",
    "        for node in sink_nodes.flatten():\n",
    "            self.graph.add_tedge(node, 0, float('inf'))\n",
    "        source_inds = np.where(self.hull_surf == 1)\n",
    "        source_nodes = self.node_ids[source_inds]\n",
    "        Wb = self._lambda * np.prod(self.voxel_size)\n",
    "        for node in source_nodes.flatten():\n",
    "            self.graph.add_tedge(node, Wb, 0)\n",
    "\n",
    "    def add_n_links(self):\n",
    "        nx, ny, nz = self.volume.shape\n",
    "        sigma = 30\n",
    "\n",
    "        for x in range(nx):\n",
    "            for y in range(ny):\n",
    "                for z in range(nz):\n",
    "                    if self.volume[x, y, z] > 0:\n",
    "                        node = self.node_ids[x, y, z]\n",
    "                        I_i = self.intensities[x, y, z, :]\n",
    "                        for dx, dy, dz in [(-1, 0, 0), (0, -1, 0), (0, 0, -1)]:\n",
    "                            nx_ = x + dx\n",
    "                            ny_ = y + dy\n",
    "                            nz_ = z + dz\n",
    "                            if nx_ >= 0 and ny_ >= 0 and nz_ >= 0 and self.volume[nx_, ny_, nz_] > 0:\n",
    "                                neighbor_node = self.node_ids[nx_, ny_, nz_]\n",
    "                                I_j = self.intensities[nx_, ny_, nz_, :]\n",
    "                                D_ij = np.linalg.norm(I_i - I_j) ** 2\n",
    "                                W = np.exp(- D_ij / (2 * sigma ** 2))\n",
    "                                self.graph.add_edge(node, neighbor_node, W, W)\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Computing Graph Cut\")\n",
    "        self.graph.maxflow()\n",
    "        self.segmentation = self.graph.get_grid_segments(self.node_ids)\n",
    "\n",
    "    def get_refined_volume(self):\n",
    "        return self.segmentation.astype(np.uint8)\n",
    "\n",
    "vg = VolumetricGraph(\n",
    "    volume = volume,\n",
    "    centers = centers,\n",
    "    intensities = intensities,\n",
    "    voxel_size = step,\n",
    "    _lambda = 1e6\n",
    ")\n",
    "vg.run()\n",
    "refined_volume = vg.get_refined_volume()\n",
    "\n",
    "data = {\n",
    "    \"version\": \"Voxel Builder 4.5.1\",\n",
    "    \"project\": {\n",
    "        \"name\": \"naruto\",\n",
    "        \"voxels\": int(np.sum(refined_volume))\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"voxels\": \";\".join([f\"{v[0]},{v[1]},{v[2]},#EE4B2B,1\" for v in zip(*np.where(refined_volume))])\n",
    "    }\n",
    "}\n",
    "with open('refined_model.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424d3fa-644c-41fa-93dc-cd68a8c207f7",
   "metadata": {},
   "source": [
    "## Shading the Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a02c0-6cef-4580-afe5-f8b54f243459",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.zeros((*list(n_vxls), 4), dtype=np.float32)\n",
    "\n",
    "for image, pose, mask in tqdm(zip(images, poses, masks), total=len(images)):\n",
    "    image = np.array(image).astype(np.float32)\n",
    "    mask = np.array(mask)\n",
    "    seg = np.where(mask > 0)\n",
    "    y, x = seg[0], seg[1]\n",
    "\n",
    "    dx = (x.astype(np.float32) - _cx) / _fx\n",
    "    dy = (y.astype(np.float32) - _cy) / _fy\n",
    "    dz = np.ones_like(dx, dtype=np.float32)\n",
    "\n",
    "    rays = (pose[:3, :3] @ scene.T @ np.vstack((dx, dy, dz))).T\n",
    "    casting = np.ones(len(dx), dtype=bool)\n",
    "\n",
    "    for z in np.arange(2000) * step.min() / 2:\n",
    "\n",
    "        points = pose[:3, :3] @ cam_pos + rays * z\n",
    "\n",
    "        voxel_coords = np.floor((points - bounds[0]) / step).astype(int)\n",
    "\n",
    "        valid_mask = ((voxel_coords >= 0) & (voxel_coords < n_vxls)).all(axis=1)\n",
    "        valid_voxels = voxel_coords[valid_mask]\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "        if len(valid_voxels) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        surface_voxels = volume[valid_voxels[:, 0], valid_voxels[:, 1], valid_voxels[:, 2]] >= len(images)\n",
    "        surf_voxel_indices = valid_indices[surface_voxels]\n",
    "\n",
    "        if len(surf_voxel_indices) > 0:\n",
    "            sv_coords = valid_voxels[surface_voxels]\n",
    "            colors = image[y[surf_voxel_indices], x[surf_voxel_indices]]\n",
    "            for idx, coord in enumerate(sv_coords):\n",
    "                i, j, k = coord\n",
    "                intensities[i, j, k, :3] += colors[idx]\n",
    "                intensities[i, j, k, 3] += 1\n",
    "\n",
    "        casting[valid_indices] &= ~surface_voxels\n",
    "        if not casting.any():\n",
    "            break\n",
    "\n",
    "non_zero_counts = intensities[..., 3] > 0\n",
    "intensities[non_zero_counts, :3] /= intensities[non_zero_counts, 3, None]\n",
    "\n",
    "intensities = np.clip(np.rint(intensities[:, :, :, :3]), a_min=0, a_max=255).astype(int)\n",
    "\n",
    "obj_mask = volume >= len(images)\n",
    "\n",
    "data = {\n",
    "    \"version\": \"Voxel Builder 4.5.1\",\n",
    "    \"project\": {\n",
    "        \"name\": \"naruto\",\n",
    "        \"voxels\": int(np.sum(obj_mask))\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"voxels\": \";\".join([f\"{v[0]},{v[1]},{v[2]},{rgb_to_hex(intensities[v[0], v[1], v[2]])},1\" for v in zip(*np.where(obj_mask))])\n",
    "    }\n",
    "}\n",
    "with open('shaded_model.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dae42c-93c1-4a44-86b3-7a7b961062e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Computing Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45aa24-aee5-4ce4-b1cf-c3cbba652732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxelize the ground truth mesh\n",
    "voxel_size = step[0]  # Assuming uniform voxel size in all dimensions\n",
    "gt_volume = mesh.voxelized(pitch=voxel_size)\n",
    "gt_volume_dense = gt_volume.matrix.astype(bool)\n",
    "\n",
    "# Align the ground truth volume to the reconstruction volume\n",
    "# This may require adjusting the origin and orientation\n",
    "gt_volume_dense_aligned = align_volumes(gt_volume_dense, obj_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe78ee9-0e2b-4790-b4f3-ca57ce431481",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_volume_dense.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
